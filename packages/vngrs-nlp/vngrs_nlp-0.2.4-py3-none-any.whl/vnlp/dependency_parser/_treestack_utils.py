import numpy as np
import tensorflow as tf

from ..normalizer import Normalizer

sentence_max_len = 40
tag_max_len = 15

num_unq_labels = 48  # len(tokenizer_label.word_index)
num_unq_pos_tags = 17  # len(tokenizer_pos.word_index)


def create_dependency_parser_model(
    word_embedding_vocab_size,
    word_embedding_vector_size,
    word_embedding_matrix,
    pos_vocab_size,
    pos_embedding_vector_size,
    sentence_max_len,
    tag_max_len,
    arc_label_vector_len,
    num_rnn_stacks,
    tag_num_rnn_units,
    lc_num_rnn_units,
    lc_arc_label_num_rnn_units,
    rc_num_rnn_units,
    dropout,
    tag_embedding_matrix,
    fc_units_multipliers,
):
    word_embedding_layer = tf.keras.layers.Embedding(
        input_dim=word_embedding_vocab_size,
        output_dim=word_embedding_vector_size,
        embeddings_initializer=tf.keras.initializers.Constant(
            word_embedding_matrix
        ),
        trainable=False,
    )
    pos_embedding_layer = tf.keras.layers.Embedding(
        input_dim=pos_vocab_size + 1, output_dim=pos_embedding_vector_size
    )

    # ===============================================
    # CURRENT WORD
    # Word
    word_input = tf.keras.layers.Input(shape=(1))
    word_embedded_ = word_embedding_layer(
        word_input
    )  # shape: (None, 1, embed_size)
    word_embedded = tf.keras.backend.squeeze(
        word_embedded_, axis=1
    )  # shape: (None, embed_size)

    # Tag
    # Transferred from learned weights of StemmerAnalyzer - Morphological Disambiguator
    # tag_embedding_matrix = sa.model.layers[5].weights[0].numpy().copy() # shape: (127, 32)
    tag_vocab_size = tag_embedding_matrix.shape[0]
    tag_embed_size = tag_embedding_matrix.shape[1]

    tag_input = tf.keras.layers.Input(shape=(tag_max_len))
    tag_embedding_layer = tf.keras.layers.Embedding(
        input_dim=tag_vocab_size,
        output_dim=tag_embed_size,
        weights=[tag_embedding_matrix],
        trainable=False,
    )
    tag_embedded = tag_embedding_layer(tag_input)

    tag_rnn = tf.keras.models.Sequential()
    for n in range(num_rnn_stacks - 1):
        tag_rnn.add(
            tf.keras.layers.GRU(tag_num_rnn_units, return_sequences=True)
        )
        tag_rnn.add(tf.keras.layers.Dropout(dropout))
    tag_rnn.add(tf.keras.layers.GRU(tag_num_rnn_units))
    tag_rnn.add(tf.keras.layers.Dropout(dropout))
    tag_rnn_output = tag_rnn(tag_embedded)

    # Pos
    pos_input = tf.keras.layers.Input(shape=(1))
    pos_embedded_ = pos_embedding_layer(pos_input)
    pos_embedded = tf.keras.backend.squeeze(pos_embedded_, axis=1)

    word_tag_pos_concatanated = tf.keras.layers.Concatenate()(
        [word_embedded, tag_rnn_output, pos_embedded]
    )

    # ===============================================
    # LEFT CONTEXT
    # Left Context Word
    left_context_word_input = tf.keras.layers.Input(shape=(sentence_max_len))
    left_context_word_embedded = word_embedding_layer(left_context_word_input)

    # Left Context Tags
    left_context_tag_input = tf.keras.layers.Input(
        shape=(sentence_max_len, tag_max_len)
    )
    left_context_tag_embedded = tag_embedding_layer(left_context_tag_input)

    left_context_td_tag_rnn_output = tf.keras.layers.TimeDistributed(
        tag_rnn, input_shape=(sentence_max_len, tag_max_len, tag_embed_size)
    )(left_context_tag_embedded)

    # Left Context POS Tags
    left_context_pos_input = tf.keras.layers.Input(shape=(sentence_max_len))
    left_context_pos_embedded = pos_embedding_layer(left_context_pos_input)

    left_context_word_td_tags_pos_concatenated = tf.keras.layers.Concatenate()(
        [
            left_context_word_embedded,
            left_context_td_tag_rnn_output,
            left_context_pos_embedded,
        ]
    )

    # Left Context Word-Tag-POS Final RNN (Left to Right)
    lc_rnn = tf.keras.models.Sequential()
    for n in range(num_rnn_stacks - 1):
        lc_rnn.add(
            tf.keras.layers.GRU(lc_num_rnn_units, return_sequences=True)
        )
        lc_rnn.add(tf.keras.layers.Dropout(dropout))
    lc_rnn.add(tf.keras.layers.GRU(lc_num_rnn_units))
    lc_rnn.add(tf.keras.layers.Dropout(dropout))

    lc_output = lc_rnn(left_context_word_td_tags_pos_concatenated)

    # Left-Context Arc-Label (previously processed tokens' arc-label results)
    lc_arc_label_input = tf.keras.layers.Input(
        shape=(sentence_max_len, arc_label_vector_len)
    )
    lc_arc_label_rnn = tf.keras.models.Sequential()
    for n in range(num_rnn_stacks - 1):
        lc_arc_label_rnn.add(
            tf.keras.layers.GRU(
                lc_arc_label_num_rnn_units, return_sequences=True
            )
        )
        lc_arc_label_rnn.add(tf.keras.layers.Dropout(dropout))
    lc_arc_label_rnn.add(tf.keras.layers.GRU(lc_arc_label_num_rnn_units))
    lc_arc_label_rnn.add(tf.keras.layers.Dropout(dropout))
    lc_arc_label_rnn_output = lc_arc_label_rnn(lc_arc_label_input)

    # ===============================================
    # RIGHT CONTEXT
    # Right Context Word
    right_context_word_input = tf.keras.layers.Input(shape=(sentence_max_len))
    right_context_word_embedded = word_embedding_layer(
        right_context_word_input
    )

    # Right Context Tags
    right_context_tag_input = tf.keras.layers.Input(
        shape=(sentence_max_len, tag_max_len)
    )
    right_context_tag_embedded = tag_embedding_layer(right_context_tag_input)

    # Right Context POS Tags
    right_context_pos_input = tf.keras.layers.Input(shape=(sentence_max_len))
    right_context_pos_embedded = pos_embedding_layer(right_context_pos_input)

    right_context_td_tag_rnn_output = tf.keras.layers.TimeDistributed(
        tag_rnn, input_shape=(sentence_max_len, tag_max_len, tag_embed_size)
    )(right_context_tag_embedded)

    right_context_word_td_tags_pos_concatenated = (
        tf.keras.layers.Concatenate()(
            [
                right_context_word_embedded,
                right_context_td_tag_rnn_output,
                right_context_pos_embedded,
            ]
        )
    )

    # Right Context Word-Tag-POS Final RNN (Right to Left)
    rc_rnn = tf.keras.models.Sequential()
    for n in range(num_rnn_stacks - 1):
        rc_rnn.add(
            tf.keras.layers.GRU(
                rc_num_rnn_units, return_sequences=True, go_backwards=True
            )
        )  # Right to Left!
        rc_rnn.add(tf.keras.layers.Dropout(dropout))
    rc_rnn.add(tf.keras.layers.GRU(rc_num_rnn_units, go_backwards=True))
    rc_rnn.add(tf.keras.layers.Dropout(dropout))

    rc_output = rc_rnn(right_context_word_td_tags_pos_concatenated)
    current_left_right_concat = tf.keras.layers.Concatenate()(
        [
            word_tag_pos_concatanated,
            lc_output,
            lc_arc_label_rnn_output,
            rc_output,
        ]
    )

    # ===============================================
    # FC LAYERS
    fc_layer_one = tf.keras.layers.Dense(
        tag_num_rnn_units * fc_units_multipliers[0], activation="relu"
    )(current_left_right_concat)
    fc_layer_one = tf.keras.layers.Dropout(dropout)(fc_layer_one)
    fc_layer_two = tf.keras.layers.Dense(
        tag_num_rnn_units * fc_units_multipliers[1], activation="relu"
    )(fc_layer_one)
    fc_layer_two = tf.keras.layers.Dropout(dropout)(fc_layer_two)
    arc_label_output = tf.keras.layers.Dense(
        arc_label_vector_len, activation="sigmoid"
    )(fc_layer_two)

    model = tf.keras.models.Model(
        inputs=[
            word_input,
            tag_input,
            pos_input,
            left_context_word_input,
            left_context_tag_input,
            left_context_pos_input,
            lc_arc_label_input,
            right_context_word_input,
            right_context_tag_input,
            right_context_pos_input,
        ],
        outputs=[arc_label_output],
    )

    return model


def preprocess_word(word):
    word = word.replace("â€™", "'")
    word = Normalizer.lower_case(word)
    word = convert_numbers_to_zero(word)
    return word


def process_single_word_input(
    t,
    whole_words_in_sentence,
    sentence_analysis_result,
    sentence_max_len,
    tag_max_len,
    arc_label_vector_len,
    tokenizer_word,
    tokenizer_tag,
    tokenizer_label,
    tokenizer_pos,
    arcs,
    labels,
    pos_tags,
    word_form,
):
    num_tokens_in_sentence = len(sentence_analysis_result)
    analysis_result = sentence_analysis_result[t]
    word_and_tags = analysis_result.split("+")

    stems = [
        analysis_result.split("+")[0]
        for analysis_result in sentence_analysis_result
    ]

    # In case a word does not exist in tokenizer_word dict,
    # its stem is used instead of an OOV token.
    # vice versa for stem case: in case a stem is not recognized
    # whole word is used
    if word_form == "stem":
        words = []
        for stem, whole_word in zip(stems, whole_words_in_sentence):
            # use whole_word only if stem is not in vocab, but whole_word IS in vocab
            # otherwise use stem anyway
            if (not (stem in tokenizer_word.word_index)) and (
                whole_word in tokenizer_word.word_index
            ):
                words.append(whole_word)
            else:
                words.append(stem)

    elif word_form == "whole":
        words = []
        for whole_word, stem in zip(whole_words_in_sentence, stems):
            # use stem only if whole_word is not in vocab, but stem IS in vocab
            # otherwise use whole_word anyway (can benefit FastText or similar morphological embedding in future)
            if (not (whole_word in tokenizer_word.word_index)) and (
                stem in tokenizer_word.word_index
            ):
                words.append(stem)
            else:
                words.append(whole_word)

    words = [preprocess_word(word) for word in words]

    word = words[t]
    word = tokenizer_word.texts_to_sequences(
        [[word]]
    )  # this wont have padding

    tags = word_and_tags[1:]
    tags = tokenizer_tag.texts_to_sequences([tags])
    tags = tf.keras.preprocessing.sequence.pad_sequences(
        tags, maxlen=tag_max_len, padding="pre"
    )[0]

    pos_tags = tokenizer_pos.texts_to_sequences([pos_tags])[0]
    pos = pos_tags[t]

    # ===============================================
    # Left Context
    if t == 0:  # there's no left context yet
        left_context_words = np.zeros(sentence_max_len)
        left_context_tags = np.zeros((sentence_max_len, tag_max_len))
        left_context_pos_tags = np.zeros(sentence_max_len)
        left_context_arc_label_vectors = np.zeros(
            (sentence_max_len, arc_label_vector_len)
        )
    else:
        left_context_words = words[:t]
        left_context_words = tokenizer_word.texts_to_sequences(
            [left_context_words]
        )
        left_context_words = tf.keras.preprocessing.sequence.pad_sequences(
            left_context_words,
            maxlen=sentence_max_len,
            truncating="pre",
            padding="pre",
        )[0]

        left_context_tags = [
            analysis_result.split("+")[1:]
            for analysis_result in sentence_analysis_result[:t]
        ]
        left_context_tags = tokenizer_tag.texts_to_sequences(left_context_tags)
        left_context_tags = tf.keras.preprocessing.sequence.pad_sequences(
            left_context_tags, maxlen=tag_max_len, padding="pre"
        )

        left_context_pos_tags = pos_tags[:t]
        left_context_pos_tags = tf.keras.preprocessing.sequence.pad_sequences(
            [left_context_pos_tags], maxlen=sentence_max_len, padding="pre"
        )

        left_context_arcs = arcs[:t]
        left_context_labels = labels[:t]

        left_context_arc_label_vectors = []
        for left_context_arc_, left_context_label_ in zip(
            left_context_arcs, left_context_labels
        ):
            left_context_one_hot_arc = tf.keras.utils.to_categorical(
                left_context_arc_, num_classes=sentence_max_len + 1
            )
            left_context_one_hot_label = tf.keras.utils.to_categorical(
                left_context_label_,
                num_classes=len(tokenizer_label.word_index) + 1,
            )

            left_context_arc_label_vector = (
                left_context_one_hot_arc.tolist()
                + left_context_one_hot_label.tolist()
            )
            left_context_arc_label_vectors.append(
                left_context_arc_label_vector
            )

        # 2D PRE-padding for left context tags and left_context_arc_vectors
        # final shapes will be:
        # left_context_tags: (sentence_max_len, tag_max_len)
        # left_context_arc_vectors: (sentence_max_len, sentence_max_len)
        left_context_tags_ = []
        left_context_tags = left_context_tags.tolist()

        left_context_arc_label_vectors_ = []
        for _ in range(max(0, sentence_max_len - len(left_context_tags))):
            left_context_tags_.append(np.zeros(tag_max_len))
            left_context_arc_label_vectors_.append(
                np.zeros(arc_label_vector_len)
            )
        left_context_tags = np.array(left_context_tags_ + left_context_tags)
        left_context_arc_label_vectors = np.array(
            left_context_arc_label_vectors_ + left_context_arc_label_vectors
        )
    # ===============================================
    # Right Context
    if t == (
        num_tokens_in_sentence - 1
    ):  # there's no right context for last token
        right_context_words = np.zeros(sentence_max_len)
        right_context_tags = np.zeros((sentence_max_len, tag_max_len))
        right_context_pos_tags = np.zeros(sentence_max_len)
    else:
        right_context_words = words[t + 1 :]
        right_context_words = tokenizer_word.texts_to_sequences(
            [right_context_words]
        )
        right_context_words = tf.keras.preprocessing.sequence.pad_sequences(
            right_context_words,
            maxlen=sentence_max_len,
            truncating="post",
            padding="post",
        )[0]

        right_context_tags = [
            analysis_result.split("+")[1:]
            for analysis_result in sentence_analysis_result[t + 1 :]
        ]
        right_context_tags = tokenizer_tag.texts_to_sequences(
            right_context_tags
        )
        right_context_tags = tf.keras.preprocessing.sequence.pad_sequences(
            right_context_tags, maxlen=tag_max_len, padding="pre"
        )  # last dimension padding is pre
        right_context_pos_tags = pos_tags[t + 1 :]
        right_context_pos_tags = tf.keras.preprocessing.sequence.pad_sequences(
            [right_context_pos_tags], maxlen=sentence_max_len, padding="post"
        )[0]

        # 2D POST-padding for right context tags
        # final shape will be: (sentence_max_len, tag_max_len)
        right_context_tags = right_context_tags.tolist()
        for _ in range(max(0, sentence_max_len - len(right_context_tags))):
            right_context_tags.append(np.zeros(tag_max_len))
        right_context_tags = np.array(right_context_tags)

    word = np.array(word[0]).reshape(1, 1).astype(int)
    tags = tags.reshape(1, tag_max_len).astype(int)
    pos = np.array([pos]).reshape(1, 1).astype(int)

    left_context_words = left_context_words.reshape(
        1, sentence_max_len
    ).astype(int)
    left_context_tags = left_context_tags.reshape(
        1, sentence_max_len, tag_max_len
    ).astype(int)
    left_context_pos_tags = left_context_pos_tags.reshape(
        1, sentence_max_len
    ).astype(int)
    left_context_arc_label_vectors = left_context_arc_label_vectors.reshape(
        1, sentence_max_len, arc_label_vector_len
    ).astype(int)

    right_context_words = right_context_words.reshape(
        1, sentence_max_len
    ).astype(int)
    right_context_tags = right_context_tags.reshape(
        1, sentence_max_len, tag_max_len
    ).astype(int)
    right_context_pos_tags = right_context_pos_tags.reshape(
        1, sentence_max_len
    ).astype(int)

    return (
        word,
        tags,
        pos,
        left_context_words,
        left_context_tags,
        left_context_pos_tags,
        left_context_arc_label_vectors,
        right_context_words,
        right_context_tags,
        right_context_pos_tags,
    )


def convert_numbers_to_zero(text_: str):
    text_ = str(text_)  # in case input is not string
    text = ""
    for char in text_:
        if char.isnumeric():
            text += "0"
        else:
            text += char
    return text
